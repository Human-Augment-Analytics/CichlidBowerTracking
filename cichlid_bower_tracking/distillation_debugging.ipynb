{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_distillation.models.transformer.feature_extractors.triplet_cross_attention_vit import TripletCrossAttentionViT as TCAiT\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug T-CAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "NUM_CLASSES = 21841\n",
    "NUM_EXTRACTOR_HEADS = 8\n",
    "NUM_CLASSIFIER_HEADS = 8\n",
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "USE_MINIPATCH = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, extractor_use_minipatch=USE_MINIPATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "\n",
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==========================================================================================\n",
      "Name                                               | Params       | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                          |       393216 | (512, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                            |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                    |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                  |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                  |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                   |       100864 | (1, 197, 512)       \n",
      "positive_pos_encoder.pos_embedding                 |       100864 | (1, 197, 512)       \n",
      "negative_pos_encoder.pos_embedding                 |       100864 | (1, 197, 512)       \n",
      "transformer_blocks.0.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.0.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.0.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.0.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.0.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.0.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.0.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.0.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.0.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.0.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.1.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.1.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.1.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.1.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.1.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.1.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.2.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.2.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.2.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.2.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.2.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.2.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.3.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.3.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.3.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.3.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.3.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.3.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.4.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.4.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.4.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.4.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.4.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.4.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.5.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.5.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.5.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.5.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.5.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.5.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.6.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.6.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.6.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.6.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.6.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.6.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.7.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.7.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.7.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.7.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.7.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.7.mlp.2.bias                    |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight       |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias         |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight      |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias        |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                    |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                      |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight       |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias         |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight      |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias        |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                    |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                      |          512 | (512,)              \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                           |                            28020224\n",
      "\n",
      "Classifier\n",
      "==========================================================================================\n",
      "Name                                               | # Params     | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                              |          512 | (512,)              \n",
      "blocks.0.norm1.bias                                |          512 | (512,)              \n",
      "blocks.0.mha.in_proj_weight                        |       786432 | (1536, 512)         \n",
      "blocks.0.mha.in_proj_bias                          |         1536 | (1536,)             \n",
      "blocks.0.mha.out_proj.weight                       |       262144 | (512, 512)          \n",
      "blocks.0.mha.out_proj.bias                         |          512 | (512,)              \n",
      "blocks.0.norm2.weight                              |          512 | (512,)              \n",
      "blocks.0.norm2.bias                                |          512 | (512,)              \n",
      "blocks.0.mlp.0.weight                              |      1048576 | (2048, 512)         \n",
      "blocks.0.mlp.0.bias                                |         2048 | (2048,)             \n",
      "blocks.0.mlp.2.weight                              |      1048576 | (512, 2048)         \n",
      "blocks.0.mlp.2.bias                                |          512 | (512,)              \n",
      "blocks.1.norm1.weight                              |          512 | (512,)              \n",
      "blocks.1.norm1.bias                                |          512 | (512,)              \n",
      "blocks.1.mha.in_proj_weight                        |       786432 | (1536, 512)         \n",
      "blocks.1.mha.in_proj_bias                          |         1536 | (1536,)             \n",
      "blocks.1.mha.out_proj.weight                       |       262144 | (512, 512)          \n",
      "blocks.1.mha.out_proj.bias                         |          512 | (512,)              \n",
      "blocks.1.norm2.weight                              |          512 | (512,)              \n",
      "blocks.1.norm2.bias                                |          512 | (512,)              \n",
      "blocks.1.mlp.0.weight                              |      1048576 | (2048, 512)         \n",
      "blocks.1.mlp.0.bias                                |         2048 | (2048,)             \n",
      "blocks.1.mlp.2.weight                              |      1048576 | (512, 2048)         \n",
      "blocks.1.mlp.2.bias                                |          512 | (512,)              \n",
      "mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "mlp.0.bias                                         |         2048 | (2048,)             \n",
      "mlp.1.weight                                       |         2048 | (2048,)             \n",
      "mlp.1.bias                                         |         2048 | (2048,)             \n",
      "mlp.3.weight                                       |     16777216 | (8192, 2048)        \n",
      "mlp.3.bias                                         |         8192 | (8192,)             \n",
      "mlp.4.weight                                       |         8192 | (8192,)             \n",
      "mlp.4.bias                                         |         8192 | (8192,)             \n",
      "mlp.6.weight                                       |    178921472 | (21841, 8192)       \n",
      "mlp.6.bias                                         |        21841 | (21841,)            \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                          |                           203104593\n",
      "\n",
      "==========================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS               |                           231124817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CichlidDistillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
