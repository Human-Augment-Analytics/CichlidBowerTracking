{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_distillation.models.transformer.feature_extractors.triplet_cross_attention_vit import TripletCrossAttentionViT as TCAiT\n",
    "from data_distillation.models.transformer.feature_extractors.pyramid.pyra_tcait import PyraTCAiT\n",
    "\n",
    "from data_distillation.losses.triplet_losses.triplet_classification_loss import TripletClassificationLoss as TCLoss\n",
    "from data_distillation.losses.triplet_losses.triplet_loss import TripletLoss\n",
    "\n",
    "from data_distillation.testing.data.test_triplets import TestTriplets\n",
    "from data_distillation.data_distiller import DataDistiller\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug TCAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "NUM_CLASSES = 21841\n",
    "NUM_EXTRACTOR_HEADS = 8\n",
    "NUM_CLASSIFIER_HEADS = 8\n",
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "USE_MINIPATCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, extractor_use_minipatch=USE_MINIPATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    anchor = anchor.cuda()\n",
    "    positive = positive.cuda()\n",
    "    negative = negative.cuda()\n",
    "\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       393216 | (512, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                                                |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                                       |       100864 | (1, 197, 512)       \n",
      "positive_pos_encoder.pos_embedding                                     |       100864 | (1, 197, 512)       \n",
      "negative_pos_encoder.pos_embedding                                     |       100864 | (1, 197, 512)       \n",
      "transformer_blocks.0.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.0.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.0.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.0.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.0.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.0.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.0.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.0.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.0.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.0.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.1.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.1.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.1.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.1.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.1.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.1.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.2.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.2.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.2.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.2.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.2.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.2.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.3.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.3.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.3.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.3.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.3.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.3.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.4.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.4.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.4.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.4.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.4.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.4.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.5.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.5.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.5.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.5.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.5.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.5.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.6.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.6.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.6.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.6.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.6.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.6.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.7.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.7.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.7.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.7.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.7.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.7.mlp.2.bias                                        |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                                               |                            28020224\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | # Params     | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                                                  |          512 | (512,)              \n",
      "blocks.0.norm1.bias                                                    |          512 | (512,)              \n",
      "blocks.0.attention.in_proj_weight                                      |       786432 | (1536, 512)         \n",
      "blocks.0.attention.in_proj_bias                                        |         1536 | (1536,)             \n",
      "blocks.0.attention.out_proj.weight                                     |       262144 | (512, 512)          \n",
      "blocks.0.attention.out_proj.bias                                       |          512 | (512,)              \n",
      "blocks.0.norm2.weight                                                  |          512 | (512,)              \n",
      "blocks.0.norm2.bias                                                    |          512 | (512,)              \n",
      "blocks.0.mlp.0.weight                                                  |      1048576 | (2048, 512)         \n",
      "blocks.0.mlp.0.bias                                                    |         2048 | (2048,)             \n",
      "blocks.0.mlp.2.weight                                                  |      1048576 | (512, 2048)         \n",
      "blocks.0.mlp.2.bias                                                    |          512 | (512,)              \n",
      "blocks.1.norm1.weight                                                  |          512 | (512,)              \n",
      "blocks.1.norm1.bias                                                    |          512 | (512,)              \n",
      "blocks.1.attention.in_proj_weight                                      |       786432 | (1536, 512)         \n",
      "blocks.1.attention.in_proj_bias                                        |         1536 | (1536,)             \n",
      "blocks.1.attention.out_proj.weight                                     |       262144 | (512, 512)          \n",
      "blocks.1.attention.out_proj.bias                                       |          512 | (512,)              \n",
      "blocks.1.norm2.weight                                                  |          512 | (512,)              \n",
      "blocks.1.norm2.bias                                                    |          512 | (512,)              \n",
      "blocks.1.mlp.0.weight                                                  |      1048576 | (2048, 512)         \n",
      "blocks.1.mlp.0.bias                                                    |         2048 | (2048,)             \n",
      "blocks.1.mlp.2.weight                                                  |      1048576 | (512, 2048)         \n",
      "blocks.1.mlp.2.bias                                                    |          512 | (512,)              \n",
      "mlp.0.weight                                                           |      1048576 | (2048, 512)         \n",
      "mlp.0.bias                                                             |         2048 | (2048,)             \n",
      "mlp.1.weight                                                           |         2048 | (2048,)             \n",
      "mlp.1.bias                                                             |         2048 | (2048,)             \n",
      "mlp.3.weight                                                           |     16777216 | (8192, 2048)        \n",
      "mlp.3.bias                                                             |         8192 | (8192,)             \n",
      "mlp.4.weight                                                           |         8192 | (8192,)             \n",
      "mlp.4.bias                                                             |         8192 | (8192,)             \n",
      "mlp.6.weight                                                           |    178921472 | (21841, 8192)       \n",
      "mlp.6.bias                                                             |        21841 | (21841,)            \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                                              |                           203104593\n",
      "\n",
      "==============================================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS                                   |                           231124817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test string function\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug PyraTCAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [64, 128, 320, 512]\n",
    "HEAD_COUNTS = [1, 2, 5, 8]\n",
    "MLP_RATIOS = [8, 8, 4, 4]\n",
    "SR_RATIOS = [8, 4, 2, 1]\n",
    "DEPTHS = [3, 3, 6, 3]\n",
    "ADD_CLASSIFIER = True\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = PyraTCAiT(embed_dims=EMBED_DIMS, head_counts=HEAD_COUNTS, mlp_ratios=MLP_RATIOS, sr_ratios=SR_RATIOS, depths=DEPTHS,\n",
    "                  add_classifier=ADD_CLASSIFIER, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    anchor = anchor.cuda()\n",
    "    positive = positive.cuda()\n",
    "    negative = negative.cuda()\n",
    "\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         3072 | (64, 3, 4, 4)       \n",
      "patcher.patch_conv.bias                                                |           64 | (64,)               \n",
      "patcher.norm.weight                                                    |           64 | (64,)               \n",
      "patcher.norm.bias                                                      |           64 | (64,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |       200704 | (1, 3136, 64)       \n",
      "positive_pos_encoder.pos_embedding                                     |       200704 | (1, 3136, 64)       \n",
      "negative_pos_encoder.pos_embedding                                     |       200704 | (1, 3136, 64)       \n",
      "transformer_stack.0.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.0.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.0.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.0.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.0.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.0.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.0.mlp.2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.1.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.1.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.1.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.1.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.1.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.1.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.1.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.1.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.1.mlp.2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.2.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.2.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.2.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.2.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.2.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.2.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.2.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.2.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.2.mlp.2.bias                                         |           64 | (64,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |        12288 | (192, 64)           \n",
      "positive_cross_attn.attention.in_proj_bias                             |          192 | (192,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |         4096 | (64, 64)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           64 | (64,)               \n",
      "positive_cross_attn.norm.weight                                        |           64 | (64,)               \n",
      "positive_cross_attn.norm.bias                                          |           64 | (64,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |        12288 | (192, 64)           \n",
      "negative_cross_attn.attention.in_proj_bias                             |          192 | (192,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |         4096 | (64, 64)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           64 | (64,)               \n",
      "negative_cross_attn.norm.weight                                        |           64 | (64,)               \n",
      "negative_cross_attn.norm.bias                                          |           64 | (64,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 0 TOTAL # PARAMS                                                 |                             1674944\n",
      "\n",
      "Stage 1\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        32768 | (128, 64, 2, 2)     \n",
      "patcher.patch_conv.bias                                                |          128 | (128,)              \n",
      "patcher.norm.weight                                                    |          128 | (128,)              \n",
      "patcher.norm.bias                                                      |          128 | (128,)              \n",
      "anchor_pos_encoder.pos_embedding                                       |       100352 | (1, 784, 128)       \n",
      "positive_pos_encoder.pos_embedding                                     |       100352 | (1, 784, 128)       \n",
      "negative_pos_encoder.pos_embedding                                     |       100352 | (1, 784, 128)       \n",
      "transformer_stack.0.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.0.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.0.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.0.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.0.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.0.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.0.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.0.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.1.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.1.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.1.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.1.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.1.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.1.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.1.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.2.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.2.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.2.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.2.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.2.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.2.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.2.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          128 | (128,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "positive_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "positive_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "negative_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "negative_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 1 TOTAL # PARAMS                                                 |                             2243968\n",
      "\n",
      "Stage 2\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       163840 | (320, 128, 2, 2)    \n",
      "patcher.patch_conv.bias                                                |          320 | (320,)              \n",
      "patcher.norm.weight                                                    |          320 | (320,)              \n",
      "patcher.norm.bias                                                      |          320 | (320,)              \n",
      "anchor_pos_encoder.pos_embedding                                       |        62720 | (1, 196, 320)       \n",
      "positive_pos_encoder.pos_embedding                                     |        62720 | (1, 196, 320)       \n",
      "negative_pos_encoder.pos_embedding                                     |        62720 | (1, 196, 320)       \n",
      "transformer_stack.0.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.0.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.0.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.0.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.0.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.0.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.0.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.0.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.1.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.1.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.1.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.1.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.1.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.1.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.1.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.2.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.2.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.2.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.2.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.2.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.2.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.2.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.3.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.3.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.3.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.3.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.3.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.3.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.3.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.3.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.3.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.3.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.3.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.3.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.3.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.4.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.4.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.4.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.4.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.4.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.4.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.4.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.4.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.4.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.4.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.4.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.4.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.4.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.5.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.5.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.5.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.5.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.5.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.5.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.5.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.5.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.5.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.5.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.5.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.5.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.5.mlp.2.bias                                         |          320 | (320,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       307200 | (960, 320)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          960 | (960,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |       102400 | (320, 320)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          320 | (320,)              \n",
      "positive_cross_attn.norm.weight                                        |          320 | (320,)              \n",
      "positive_cross_attn.norm.bias                                          |          320 | (320,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       307200 | (960, 320)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          960 | (960,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |       102400 | (320, 320)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          320 | (320,)              \n",
      "negative_cross_attn.norm.weight                                        |          320 | (320,)              \n",
      "negative_cross_attn.norm.bias                                          |          320 | (320,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 2 TOTAL # PARAMS                                                 |                            11037120\n",
      "\n",
      "Stage 3\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       655360 | (512, 320, 2, 2)    \n",
      "patcher.patch_conv.bias                                                |          512 | (512,)              \n",
      "patcher.norm.weight                                                    |          512 | (512,)              \n",
      "patcher.norm.bias                                                      |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                                       |        25600 | (1, 50, 512)        \n",
      "positive_pos_encoder.pos_embedding                                     |        25600 | (1, 50, 512)        \n",
      "negative_pos_encoder.pos_embedding                                     |        25600 | (1, 50, 512)        \n",
      "transformer_stack.0.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.0.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.0.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.0.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.1.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.1.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.1.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.2.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.2.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.2.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 3 TOTAL # PARAMS                                                 |                            12295680\n",
      "\n",
      "==============================================================================================================\n",
      "Pre-classifier PyraT-CAiT # PARAMS                                     |                            27251712\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "weight                                                                 |         1024 | (2, 512)            \n",
      "bias                                                                   |            2 | (2,)                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CLASSIFIER # PARAMS                                                    |                                1026\n",
      "\n",
      "==============================================================================================================\n",
      "FULL PyraT-CAiT # PARAMS                                               |                            27252738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test string function\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataDistiller object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "NUM_CLASSES = 2\n",
    "NUM_EXTRACTOR_HEADS = 2\n",
    "NUM_CLASSIFIER_HEADS = 2\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_BATCHES = 4\n",
    "NUM_VALID_BATCHES = 1\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "EXTRACTOR_DEPTH = 4\n",
    "EXTRACTOR_MLP_RATIO = 2.0\n",
    "CLASSIFIER_DEPTH = 1\n",
    "CLASSIFIER_MLP_RATIO = 2.0\n",
    "USE_MINIPATCH = False\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "SAVE_BEST_WEIGHTS = True\n",
    "SAVE_DIR = '/Users/charlieclark/Documents/GATech/OMSCS/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/weights'\n",
    "SAVE_FILE = 'test.pt'\n",
    "SAVE_FP = SAVE_DIR + '/' + SAVE_FILE\n",
    "DEVICE = 'cpu'\n",
    "GPU_ID = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        98304 | (128, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                                                |          128 | (128,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          128 | (1, 1, 128)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          128 | (1, 1, 128)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          128 | (1, 1, 128)         \n",
      "anchor_pos_encoder.pos_embedding                                       |        25216 | (1, 197, 128)       \n",
      "positive_pos_encoder.pos_embedding                                     |        25216 | (1, 197, 128)       \n",
      "negative_pos_encoder.pos_embedding                                     |        25216 | (1, 197, 128)       \n",
      "transformer_blocks.0.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.0.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.0.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.0.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.0.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.0.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.0.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.0.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.0.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.0.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.1.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.1.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.1.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.1.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.1.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.1.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.2.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.2.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.2.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.2.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.2.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.2.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.3.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.3.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.3.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.3.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.3.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.3.mlp.2.bias                                        |          128 | (128,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "positive_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "positive_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "negative_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "negative_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                                               |                              836992\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | # Params     | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                                                  |          128 | (128,)              \n",
      "blocks.0.norm1.bias                                                    |          128 | (128,)              \n",
      "blocks.0.attention.in_proj_weight                                      |        49152 | (384, 128)          \n",
      "blocks.0.attention.in_proj_bias                                        |          384 | (384,)              \n",
      "blocks.0.attention.out_proj.weight                                     |        16384 | (128, 128)          \n",
      "blocks.0.attention.out_proj.bias                                       |          128 | (128,)              \n",
      "blocks.0.norm2.weight                                                  |          128 | (128,)              \n",
      "blocks.0.norm2.bias                                                    |          128 | (128,)              \n",
      "blocks.0.mlp.0.weight                                                  |        32768 | (256, 128)          \n",
      "blocks.0.mlp.0.bias                                                    |          256 | (256,)              \n",
      "blocks.0.mlp.2.weight                                                  |        32768 | (128, 256)          \n",
      "blocks.0.mlp.2.bias                                                    |          128 | (128,)              \n",
      "mlp.0.weight                                                           |        32768 | (256, 128)          \n",
      "mlp.0.bias                                                             |          256 | (256,)              \n",
      "mlp.1.weight                                                           |          256 | (256,)              \n",
      "mlp.1.bias                                                             |          256 | (256,)              \n",
      "mlp.3.weight                                                           |       131072 | (512, 256)          \n",
      "mlp.3.bias                                                             |          512 | (512,)              \n",
      "mlp.4.weight                                                           |          512 | (512,)              \n",
      "mlp.4.bias                                                             |          512 | (512,)              \n",
      "mlp.6.weight                                                           |         1024 | (2, 512)            \n",
      "mlp.6.bias                                                             |            2 | (2,)                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                                              |                              299650\n",
      "\n",
      "==============================================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS                                   |                             1136642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup simple T-CAiT model\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, \\\n",
    "              extractor_depth=EXTRACTOR_DEPTH, extractor_mlp_ratio=EXTRACTOR_MLP_RATIO, classifier_depth=CLASSIFIER_DEPTH, classifier_mlp_ratio=CLASSIFIER_MLP_RATIO, extractor_use_minipatch=USE_MINIPATCH)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets and dataloaders\n",
    "train_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_TRAIN_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_VALID_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function\n",
    "loss_fn = TCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'save_best_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# set up datadistiller\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m distiller \u001b[39m=\u001b[39m DataDistiller(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, valid_dataloader\u001b[39m=\u001b[39;49mvalid_dataloader, model\u001b[39m=\u001b[39;49mmodel, loss_fn\u001b[39m=\u001b[39;49mloss_fn, optimizer\u001b[39m=\u001b[39;49moptimizer, nepochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, nclasses\u001b[39m=\u001b[39;49mNUM_CLASSES, save_best_weights\u001b[39m=\u001b[39;49mSAVE_BEST_WEIGHTS, save_fp\u001b[39m=\u001b[39;49mSAVE_FP, device\u001b[39m=\u001b[39;49mDEVICE, gpu_id\u001b[39m=\u001b[39;49mGPU_ID)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'save_best_weights'"
     ]
    }
   ],
   "source": [
    "# set up datadistiller\n",
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, nepochs=NUM_EPOCHS, nclasses=NUM_CLASSES, save_best_weights=SAVE_BEST_WEIGHTS, save_fp=SAVE_FP, device=DEVICE, gpu_id=GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [00:11<00:00,  1.66it/s, accuracy=0.518, loss=12.5]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:05<00:00,  3.47it/s, accuracy=0.548, loss=5.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [00:09<00:00,  1.98it/s, accuracy=0.473, loss=3.76]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:05<00:00,  3.36it/s, accuracy=0.51, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [2/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [00:10<00:00,  1.80it/s, accuracy=0.496, loss=0.906]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:05<00:00,  3.42it/s, accuracy=0.493, loss=0.852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [3/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [00:10<00:00,  1.78it/s, accuracy=0.453, loss=0.896]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:05<00:00,  3.42it/s, accuracy=0.5, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [4/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [00:11<00:00,  1.62it/s, accuracy=0.43, loss=0.955] \n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:05<00:00,  3.55it/s, accuracy=0.492, loss=0.816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================================\n",
      "BEST VALIDATION MODEL ACCURACY: 0.4923\n",
      "=============================================================================================\n",
      "\n",
      "Attempting to save best model weights...\n",
      "\tSave successful!\n"
     ]
    }
   ],
   "source": [
    "# perform training/validation\n",
    "distiller.run_main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [12, 24, 48, 96]\n",
    "HEAD_COUNTS = [1, 2, 4, 6]\n",
    "MLP_RATIOS = [4, 4, 2, 2]\n",
    "SR_RATIOS = [8, 4, 2, 1]\n",
    "DEPTHS = [1, 2, 4, 2]\n",
    "ADD_CLASSIFIER = True\n",
    "NUM_CLASSES = 2\n",
    "INIT_ALPHA = 0.1\n",
    "INIT_BETA = 0.1\n",
    "USE_IMPROVED = True\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_BEST_WEIGHTS = True\n",
    "SAVE_DIR = '/Users/charlieclark/Documents/GATech/OMSCS/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/weights'\n",
    "SAVE_FILE = 'test2.pt'\n",
    "SAVE_FP = SAVE_DIR + '/' + SAVE_FILE\n",
    "DEVICE = 'cpu'\n",
    "GPU_ID = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |          576 | (12, 3, 4, 4)       \n",
      "patcher.patch_conv.bias                                                |           12 | (12,)               \n",
      "patcher.norm.weight                                                    |           12 | (12,)               \n",
      "patcher.norm.bias                                                      |           12 | (12,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |        37632 | (1, 3136, 12)       \n",
      "positive_pos_encoder.pos_embedding                                     |        37632 | (1, 3136, 12)       \n",
      "negative_pos_encoder.pos_embedding                                     |        37632 | (1, 3136, 12)       \n",
      "transformer_stack.0.norm1.weight                                       |           12 | (12,)               \n",
      "transformer_stack.0.norm1.bias                                         |           12 | (12,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |          432 | (36, 12)            \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |           36 | (36,)               \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |          144 | (12, 12)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           12 | (12,)               \n",
      "transformer_stack.0.attention.sr.weight                                |         9216 | (12, 12, 8, 8)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           12 | (12,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           12 | (12,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           12 | (12,)               \n",
      "transformer_stack.0.norm2.weight                                       |           12 | (12,)               \n",
      "transformer_stack.0.norm2.bias                                         |           12 | (12,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |          576 | (48, 12)            \n",
      "transformer_stack.0.mlp.0.bias                                         |           48 | (48,)               \n",
      "transformer_stack.0.mlp.2.weight                                       |          576 | (12, 48)            \n",
      "transformer_stack.0.mlp.2.bias                                         |           12 | (12,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |          432 | (36, 12)            \n",
      "positive_cross_attn.attention.in_proj_bias                             |           36 | (36,)               \n",
      "positive_cross_attn.attention.out_proj.weight                          |          144 | (12, 12)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           12 | (12,)               \n",
      "positive_cross_attn.norm.weight                                        |           12 | (12,)               \n",
      "positive_cross_attn.norm.bias                                          |           12 | (12,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |          432 | (36, 12)            \n",
      "negative_cross_attn.attention.in_proj_bias                             |           36 | (36,)               \n",
      "negative_cross_attn.attention.out_proj.weight                          |          144 | (12, 12)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           12 | (12,)               \n",
      "negative_cross_attn.norm.weight                                        |           12 | (12,)               \n",
      "negative_cross_attn.norm.bias                                          |           12 | (12,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 0 TOTAL # PARAMS                                                 |                              125940\n",
      "\n",
      "Stage 1\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         1152 | (24, 12, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           24 | (24,)               \n",
      "patcher.norm.weight                                                    |           24 | (24,)               \n",
      "patcher.norm.bias                                                      |           24 | (24,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |        18816 | (1, 784, 24)        \n",
      "positive_pos_encoder.pos_embedding                                     |        18816 | (1, 784, 24)        \n",
      "negative_pos_encoder.pos_embedding                                     |        18816 | (1, 784, 24)        \n",
      "transformer_stack.0.norm1.weight                                       |           24 | (24,)               \n",
      "transformer_stack.0.norm1.bias                                         |           24 | (24,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |         1728 | (72, 24)            \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |           72 | (72,)               \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |          576 | (24, 24)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           24 | (24,)               \n",
      "transformer_stack.0.attention.sr.weight                                |         9216 | (24, 24, 4, 4)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           24 | (24,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           24 | (24,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           24 | (24,)               \n",
      "transformer_stack.0.norm2.weight                                       |           24 | (24,)               \n",
      "transformer_stack.0.norm2.bias                                         |           24 | (24,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |         2304 | (96, 24)            \n",
      "transformer_stack.0.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.2.weight                                       |         2304 | (24, 96)            \n",
      "transformer_stack.0.mlp.2.bias                                         |           24 | (24,)               \n",
      "transformer_stack.1.norm1.weight                                       |           24 | (24,)               \n",
      "transformer_stack.1.norm1.bias                                         |           24 | (24,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |         1728 | (72, 24)            \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |           72 | (72,)               \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |          576 | (24, 24)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           24 | (24,)               \n",
      "transformer_stack.1.attention.sr.weight                                |         9216 | (24, 24, 4, 4)      \n",
      "transformer_stack.1.attention.sr.bias                                  |           24 | (24,)               \n",
      "transformer_stack.1.attention.norm.weight                              |           24 | (24,)               \n",
      "transformer_stack.1.attention.norm.bias                                |           24 | (24,)               \n",
      "transformer_stack.1.norm2.weight                                       |           24 | (24,)               \n",
      "transformer_stack.1.norm2.bias                                         |           24 | (24,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |         2304 | (96, 24)            \n",
      "transformer_stack.1.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.mlp.2.weight                                       |         2304 | (24, 96)            \n",
      "transformer_stack.1.mlp.2.bias                                         |           24 | (24,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |         1728 | (72, 24)            \n",
      "positive_cross_attn.attention.in_proj_bias                             |           72 | (72,)               \n",
      "positive_cross_attn.attention.out_proj.weight                          |          576 | (24, 24)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           24 | (24,)               \n",
      "positive_cross_attn.norm.weight                                        |           24 | (24,)               \n",
      "positive_cross_attn.norm.bias                                          |           24 | (24,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |         1728 | (72, 24)            \n",
      "negative_cross_attn.attention.in_proj_bias                             |           72 | (72,)               \n",
      "negative_cross_attn.attention.out_proj.weight                          |          576 | (24, 24)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           24 | (24,)               \n",
      "negative_cross_attn.norm.weight                                        |           24 | (24,)               \n",
      "negative_cross_attn.norm.bias                                          |           24 | (24,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 1 TOTAL # PARAMS                                                 |                               95592\n",
      "\n",
      "Stage 2\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         4608 | (48, 24, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           48 | (48,)               \n",
      "patcher.norm.weight                                                    |           48 | (48,)               \n",
      "patcher.norm.bias                                                      |           48 | (48,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |         9408 | (1, 196, 48)        \n",
      "positive_pos_encoder.pos_embedding                                     |         9408 | (1, 196, 48)        \n",
      "negative_pos_encoder.pos_embedding                                     |         9408 | (1, 196, 48)        \n",
      "transformer_stack.0.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.0.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.0.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.0.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.0.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.0.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.0.mlp.2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.1.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.1.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.1.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.1.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.1.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.1.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.1.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.1.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.1.mlp.2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.2.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.2.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.2.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.2.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.2.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.2.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.2.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.2.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.2.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.2.mlp.2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.3.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.3.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.3.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.3.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.3.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.3.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.3.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.3.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.3.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.3.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.3.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.3.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.3.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.3.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.3.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.3.mlp.2.bias                                         |           48 | (48,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |         6912 | (144, 48)           \n",
      "positive_cross_attn.attention.in_proj_bias                             |          144 | (144,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |         2304 | (48, 48)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           48 | (48,)               \n",
      "positive_cross_attn.norm.weight                                        |           48 | (48,)               \n",
      "positive_cross_attn.norm.bias                                          |           48 | (48,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |         6912 | (144, 48)           \n",
      "negative_cross_attn.attention.in_proj_bias                             |          144 | (144,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |         2304 | (48, 48)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           48 | (48,)               \n",
      "negative_cross_attn.norm.weight                                        |           48 | (48,)               \n",
      "negative_cross_attn.norm.bias                                          |           48 | (48,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 2 TOTAL # PARAMS                                                 |                              165264\n",
      "\n",
      "Stage 3\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        18432 | (96, 48, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           96 | (96,)               \n",
      "patcher.norm.weight                                                    |           96 | (96,)               \n",
      "patcher.norm.bias                                                      |           96 | (96,)               \n",
      "anchor_cls_tokenizer.cls_tokens                                        |           96 | (1, 1, 96)          \n",
      "positive_cls_tokenizer.cls_tokens                                      |           96 | (1, 1, 96)          \n",
      "negative_cls_tokenizer.cls_tokens                                      |           96 | (1, 1, 96)          \n",
      "anchor_pos_encoder.pos_embedding                                       |         4800 | (1, 50, 96)         \n",
      "positive_pos_encoder.pos_embedding                                     |         4800 | (1, 50, 96)         \n",
      "negative_pos_encoder.pos_embedding                                     |         4800 | (1, 50, 96)         \n",
      "transformer_stack.0.norm1.weight                                       |           96 | (96,)               \n",
      "transformer_stack.0.norm1.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        27648 | (288, 96)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          288 | (288,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         9216 | (96, 96)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           96 | (96,)               \n",
      "transformer_stack.0.norm2.weight                                       |           96 | (96,)               \n",
      "transformer_stack.0.norm2.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |        18432 | (192, 96)           \n",
      "transformer_stack.0.mlp.0.bias                                         |          192 | (192,)              \n",
      "transformer_stack.0.mlp.2.weight                                       |        18432 | (96, 192)           \n",
      "transformer_stack.0.mlp.2.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.norm1.weight                                       |           96 | (96,)               \n",
      "transformer_stack.1.norm1.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |        27648 | (288, 96)           \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          288 | (288,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |         9216 | (96, 96)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           96 | (96,)               \n",
      "transformer_stack.1.norm2.weight                                       |           96 | (96,)               \n",
      "transformer_stack.1.norm2.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |        18432 | (192, 96)           \n",
      "transformer_stack.1.mlp.0.bias                                         |          192 | (192,)              \n",
      "transformer_stack.1.mlp.2.weight                                       |        18432 | (96, 192)           \n",
      "transformer_stack.1.mlp.2.bias                                         |           96 | (96,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |        27648 | (288, 96)           \n",
      "positive_cross_attn.attention.in_proj_bias                             |          288 | (288,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |         9216 | (96, 96)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           96 | (96,)               \n",
      "positive_cross_attn.norm.weight                                        |           96 | (96,)               \n",
      "positive_cross_attn.norm.bias                                          |           96 | (96,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |        27648 | (288, 96)           \n",
      "negative_cross_attn.attention.in_proj_bias                             |          288 | (288,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |         9216 | (96, 96)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           96 | (96,)               \n",
      "negative_cross_attn.norm.weight                                        |           96 | (96,)               \n",
      "negative_cross_attn.norm.bias                                          |           96 | (96,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 3 TOTAL # PARAMS                                                 |                              257856\n",
      "\n",
      "==========================================================================================\n",
      "PyraT-CAiT # PARAMS                                |                              644652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup simple PyraT-CAiT model\n",
    "model = PyraTCAiT(embed_dims=EMBED_DIMS, head_counts=HEAD_COUNTS, mlp_ratios=MLP_RATIOS, sr_ratios=SR_RATIOS, depths=DEPTHS,\n",
    "                  add_classifier=ADD_CLASSIFIER, num_classes=NUM_CLASSES, init_alpha=INIT_ALPHA, init_beta=INIT_BETA, use_improved=USE_IMPROVED)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets and dataloaders\n",
    "train_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_TRAIN_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_VALID_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function\n",
    "loss_fn = TCLoss() if ADD_CLASSIFIER else TripletLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datadistiller\n",
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, nepochs=NUM_EPOCHS, nclasses=NUM_CLASSES, save_best_weights=SAVE_BEST_WEIGHTS, save_fp=SAVE_FP, device=DEVICE, gpu_id=GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/2]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [02:23<00:00,  7.55s/it, loss=0.732]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:31<00:00,  1.67s/it, loss=0.755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/2]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|| 19/19 [01:22<00:00,  4.36s/it, loss=0.723]\n",
      "Validation, Batch [18/19]: 100%|| 19/19 [00:31<00:00,  1.64s/it, loss=0.732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================================\n",
      "BEST VALIDATION MODEL LOSS: 0.7323\n",
      "=============================================================================================\n",
      "\n",
      "Attempting to save best model weights...\n",
      "\tSave successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform training/validation\n",
    "distiller.run_main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CichlidDistillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
