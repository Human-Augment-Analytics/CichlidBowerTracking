{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_distillation.models.transformer.feature_extractors.triplet_cross_attention_vit import TripletCrossAttentionViT as TCAiT\n",
    "from data_distillation.losses.triplet_losses.triplet_classification_loss import TripletClassificationLoss as TCLoss\n",
    "\n",
    "from data_distillation.testing.data.test_triplets import TestTriplets\n",
    "from data_distillation.data_distiller import DataDistiller\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug T-CAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "NUM_CLASSES = 21841\n",
    "NUM_EXTRACTOR_HEADS = 8\n",
    "NUM_CLASSIFIER_HEADS = 8\n",
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "USE_MINIPATCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, extractor_use_minipatch=USE_MINIPATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    anchor = anchor.cuda()\n",
    "    positive = positive.cuda()\n",
    "    negative = negative.cuda()\n",
    "\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlieclark/miniconda3/envs/CichlidDistillation/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969118/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==========================================================================================\n",
      "Name                                               | Params       | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                          |       393216 | (512, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                            |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                    |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                  |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                  |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                   |       100864 | (1, 197, 512)       \n",
      "positive_pos_encoder.pos_embedding                 |       100864 | (1, 197, 512)       \n",
      "negative_pos_encoder.pos_embedding                 |       100864 | (1, 197, 512)       \n",
      "transformer_blocks.0.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.0.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.0.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.0.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.0.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.0.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.0.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.0.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.0.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.0.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.1.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.1.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.1.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.1.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.1.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.1.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.1.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.2.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.2.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.2.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.2.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.2.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.2.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.2.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.3.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.3.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.3.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.3.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.3.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.3.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.3.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.4.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.4.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.4.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.4.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.4.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.4.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.4.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.5.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.5.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.5.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.5.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.5.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.5.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.5.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.6.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.6.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.6.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.6.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.6.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.6.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.6.mlp.2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.weight                  |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.mha.in_proj_weight            |       786432 | (1536, 512)         \n",
      "transformer_blocks.7.mha.in_proj_bias              |         1536 | (1536,)             \n",
      "transformer_blocks.7.mha.out_proj.weight           |       262144 | (512, 512)          \n",
      "transformer_blocks.7.mha.out_proj.bias             |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.weight                  |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.bias                    |          512 | (512,)              \n",
      "transformer_blocks.7.mlp.0.weight                  |      1048576 | (2048, 512)         \n",
      "transformer_blocks.7.mlp.0.bias                    |         2048 | (2048,)             \n",
      "transformer_blocks.7.mlp.2.weight                  |      1048576 | (512, 2048)         \n",
      "transformer_blocks.7.mlp.2.bias                    |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight       |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias         |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight      |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias        |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                    |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                      |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight       |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias         |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight      |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias        |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                    |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                      |          512 | (512,)              \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                           |                            28020224\n",
      "\n",
      "Classifier\n",
      "==========================================================================================\n",
      "Name                                               | # Params     | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                              |          512 | (512,)              \n",
      "blocks.0.norm1.bias                                |          512 | (512,)              \n",
      "blocks.0.mha.in_proj_weight                        |       786432 | (1536, 512)         \n",
      "blocks.0.mha.in_proj_bias                          |         1536 | (1536,)             \n",
      "blocks.0.mha.out_proj.weight                       |       262144 | (512, 512)          \n",
      "blocks.0.mha.out_proj.bias                         |          512 | (512,)              \n",
      "blocks.0.norm2.weight                              |          512 | (512,)              \n",
      "blocks.0.norm2.bias                                |          512 | (512,)              \n",
      "blocks.0.mlp.0.weight                              |      1048576 | (2048, 512)         \n",
      "blocks.0.mlp.0.bias                                |         2048 | (2048,)             \n",
      "blocks.0.mlp.2.weight                              |      1048576 | (512, 2048)         \n",
      "blocks.0.mlp.2.bias                                |          512 | (512,)              \n",
      "blocks.1.norm1.weight                              |          512 | (512,)              \n",
      "blocks.1.norm1.bias                                |          512 | (512,)              \n",
      "blocks.1.mha.in_proj_weight                        |       786432 | (1536, 512)         \n",
      "blocks.1.mha.in_proj_bias                          |         1536 | (1536,)             \n",
      "blocks.1.mha.out_proj.weight                       |       262144 | (512, 512)          \n",
      "blocks.1.mha.out_proj.bias                         |          512 | (512,)              \n",
      "blocks.1.norm2.weight                              |          512 | (512,)              \n",
      "blocks.1.norm2.bias                                |          512 | (512,)              \n",
      "blocks.1.mlp.0.weight                              |      1048576 | (2048, 512)         \n",
      "blocks.1.mlp.0.bias                                |         2048 | (2048,)             \n",
      "blocks.1.mlp.2.weight                              |      1048576 | (512, 2048)         \n",
      "blocks.1.mlp.2.bias                                |          512 | (512,)              \n",
      "mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "mlp.0.bias                                         |         2048 | (2048,)             \n",
      "mlp.1.weight                                       |         2048 | (2048,)             \n",
      "mlp.1.bias                                         |         2048 | (2048,)             \n",
      "mlp.3.weight                                       |     16777216 | (8192, 2048)        \n",
      "mlp.3.bias                                         |         8192 | (8192,)             \n",
      "mlp.4.weight                                       |         8192 | (8192,)             \n",
      "mlp.4.bias                                         |         8192 | (8192,)             \n",
      "mlp.6.weight                                       |    178921472 | (21841, 8192)       \n",
      "mlp.6.bias                                         |        21841 | (21841,)            \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                          |                           203104593\n",
      "\n",
      "==========================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS               |                           231124817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataDistiller object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "NUM_CLASSES = 2\n",
    "NUM_EXTRACTOR_HEADS = 2\n",
    "NUM_CLASSIFIER_HEADS = 2\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_BATCHES = 10\n",
    "NUM_VALID_BATCHES = 2\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "EXTRACTOR_DEPTH = 4\n",
    "EXTRACTOR_MLP_RATIO = 2.0\n",
    "CLASSIFIER_DEPTH = 1\n",
    "CLASSIFIER_MLP_RATIO = 2.0\n",
    "USE_MINIPATCH = False\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "SAVE_BEST_WEIGHTS = True\n",
    "SAVE_DIR = '/Users/charlieclark/Documents/GATech/OMSCS/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/weights'\n",
    "SAVE_FILE = 'test.pt'\n",
    "SAVE_FP = SAVE_DIR + '/' + SAVE_FILE\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==========================================================================================\n",
      "Name                                               | Params       | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                          |        98304 | (128, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                            |          128 | (128,)              \n",
      "anchor_cls_tokenizer.cls_tokens                    |          128 | (1, 1, 128)         \n",
      "positive_cls_tokenizer.cls_tokens                  |          128 | (1, 1, 128)         \n",
      "negative_cls_tokenizer.cls_tokens                  |          128 | (1, 1, 128)         \n",
      "anchor_pos_encoder.pos_embedding                   |        25216 | (1, 197, 128)       \n",
      "positive_pos_encoder.pos_embedding                 |        25216 | (1, 197, 128)       \n",
      "negative_pos_encoder.pos_embedding                 |        25216 | (1, 197, 128)       \n",
      "transformer_blocks.0.norm1.weight                  |          128 | (128,)              \n",
      "transformer_blocks.0.norm1.bias                    |          128 | (128,)              \n",
      "transformer_blocks.0.mha.in_proj_weight            |        49152 | (384, 128)          \n",
      "transformer_blocks.0.mha.in_proj_bias              |          384 | (384,)              \n",
      "transformer_blocks.0.mha.out_proj.weight           |        16384 | (128, 128)          \n",
      "transformer_blocks.0.mha.out_proj.bias             |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.weight                  |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.0.mlp.0.weight                  |        32768 | (256, 128)          \n",
      "transformer_blocks.0.mlp.0.bias                    |          256 | (256,)              \n",
      "transformer_blocks.0.mlp.2.weight                  |        32768 | (128, 256)          \n",
      "transformer_blocks.0.mlp.2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.weight                  |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.bias                    |          128 | (128,)              \n",
      "transformer_blocks.1.mha.in_proj_weight            |        49152 | (384, 128)          \n",
      "transformer_blocks.1.mha.in_proj_bias              |          384 | (384,)              \n",
      "transformer_blocks.1.mha.out_proj.weight           |        16384 | (128, 128)          \n",
      "transformer_blocks.1.mha.out_proj.bias             |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.weight                  |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.1.mlp.0.weight                  |        32768 | (256, 128)          \n",
      "transformer_blocks.1.mlp.0.bias                    |          256 | (256,)              \n",
      "transformer_blocks.1.mlp.2.weight                  |        32768 | (128, 256)          \n",
      "transformer_blocks.1.mlp.2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.weight                  |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.bias                    |          128 | (128,)              \n",
      "transformer_blocks.2.mha.in_proj_weight            |        49152 | (384, 128)          \n",
      "transformer_blocks.2.mha.in_proj_bias              |          384 | (384,)              \n",
      "transformer_blocks.2.mha.out_proj.weight           |        16384 | (128, 128)          \n",
      "transformer_blocks.2.mha.out_proj.bias             |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.weight                  |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.2.mlp.0.weight                  |        32768 | (256, 128)          \n",
      "transformer_blocks.2.mlp.0.bias                    |          256 | (256,)              \n",
      "transformer_blocks.2.mlp.2.weight                  |        32768 | (128, 256)          \n",
      "transformer_blocks.2.mlp.2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.weight                  |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.bias                    |          128 | (128,)              \n",
      "transformer_blocks.3.mha.in_proj_weight            |        49152 | (384, 128)          \n",
      "transformer_blocks.3.mha.in_proj_bias              |          384 | (384,)              \n",
      "transformer_blocks.3.mha.out_proj.weight           |        16384 | (128, 128)          \n",
      "transformer_blocks.3.mha.out_proj.bias             |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.weight                  |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.bias                    |          128 | (128,)              \n",
      "transformer_blocks.3.mlp.0.weight                  |        32768 | (256, 128)          \n",
      "transformer_blocks.3.mlp.0.bias                    |          256 | (256,)              \n",
      "transformer_blocks.3.mlp.2.weight                  |        32768 | (128, 256)          \n",
      "transformer_blocks.3.mlp.2.bias                    |          128 | (128,)              \n",
      "positive_cross_attn.attention.in_proj_weight       |        49152 | (384, 128)          \n",
      "positive_cross_attn.attention.in_proj_bias         |          384 | (384,)              \n",
      "positive_cross_attn.attention.out_proj.weight      |        16384 | (128, 128)          \n",
      "positive_cross_attn.attention.out_proj.bias        |          128 | (128,)              \n",
      "positive_cross_attn.norm.weight                    |          128 | (128,)              \n",
      "positive_cross_attn.norm.bias                      |          128 | (128,)              \n",
      "negative_cross_attn.attention.in_proj_weight       |        49152 | (384, 128)          \n",
      "negative_cross_attn.attention.in_proj_bias         |          384 | (384,)              \n",
      "negative_cross_attn.attention.out_proj.weight      |        16384 | (128, 128)          \n",
      "negative_cross_attn.attention.out_proj.bias        |          128 | (128,)              \n",
      "negative_cross_attn.norm.weight                    |          128 | (128,)              \n",
      "negative_cross_attn.norm.bias                      |          128 | (128,)              \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                           |                              836992\n",
      "\n",
      "Classifier\n",
      "==========================================================================================\n",
      "Name                                               | # Params     | Size                \n",
      "------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                              |          128 | (128,)              \n",
      "blocks.0.norm1.bias                                |          128 | (128,)              \n",
      "blocks.0.mha.in_proj_weight                        |        49152 | (384, 128)          \n",
      "blocks.0.mha.in_proj_bias                          |          384 | (384,)              \n",
      "blocks.0.mha.out_proj.weight                       |        16384 | (128, 128)          \n",
      "blocks.0.mha.out_proj.bias                         |          128 | (128,)              \n",
      "blocks.0.norm2.weight                              |          128 | (128,)              \n",
      "blocks.0.norm2.bias                                |          128 | (128,)              \n",
      "blocks.0.mlp.0.weight                              |        32768 | (256, 128)          \n",
      "blocks.0.mlp.0.bias                                |          256 | (256,)              \n",
      "blocks.0.mlp.2.weight                              |        32768 | (128, 256)          \n",
      "blocks.0.mlp.2.bias                                |          128 | (128,)              \n",
      "mlp.0.weight                                       |        32768 | (256, 128)          \n",
      "mlp.0.bias                                         |          256 | (256,)              \n",
      "mlp.1.weight                                       |          256 | (256,)              \n",
      "mlp.1.bias                                         |          256 | (256,)              \n",
      "mlp.3.weight                                       |       131072 | (512, 256)          \n",
      "mlp.3.bias                                         |          512 | (512,)              \n",
      "mlp.4.weight                                       |          512 | (512,)              \n",
      "mlp.4.bias                                         |          512 | (512,)              \n",
      "mlp.6.weight                                       |         1024 | (2, 512)            \n",
      "mlp.6.bias                                         |            2 | (2,)                \n",
      "------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                          |                              299650\n",
      "\n",
      "==========================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS               |                             1136642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup simple T-CAiT model\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, \\\n",
    "              extractor_depth=EXTRACTOR_DEPTH, extractor_mlp_ratio=EXTRACTOR_MLP_RATIO, classifier_depth=CLASSIFIER_DEPTH, classifier_mlp_ratio=CLASSIFIER_MLP_RATIO, extractor_use_minipatch=USE_MINIPATCH)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets and dataloaders\n",
    "train_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_TRAIN_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_VALID_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function\n",
    "loss_fn = TCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datadistiller\n",
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, nepochs=NUM_EPOCHS, nclasses=NUM_CLASSES, save_best_weights=SAVE_BEST_WEIGHTS, save_fp=SAVE_FP, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [9/10]: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s, accuracy=0.475, loss=20.8]\n",
      "Validation, Batch [1/2]: 100%|██████████| 2/2 [00:00<00:00,  2.22it/s, accuracy=0.531, loss=13.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [9/10]: 100%|██████████| 10/10 [00:06<00:00,  1.55it/s, accuracy=0.519, loss=8.22]\n",
      "Validation, Batch [1/2]: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s, accuracy=0.625, loss=4.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [2/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [9/10]: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s, accuracy=0.506, loss=2.64]\n",
      "Validation, Batch [1/2]: 100%|██████████| 2/2 [00:00<00:00,  2.26it/s, accuracy=0.406, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [3/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [9/10]: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s, accuracy=0.506, loss=0.975]\n",
      "Validation, Batch [1/2]: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s, accuracy=0.406, loss=0.796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [4/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [9/10]: 100%|██████████| 10/10 [00:06<00:00,  1.60it/s, accuracy=0.419, loss=0.798]\n",
      "Validation, Batch [1/2]: 100%|██████████| 2/2 [00:00<00:00,  2.29it/s, accuracy=0.562, loss=0.776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================================\n",
      "BEST VALIDATION MODEL ACCURACY: 0.4062\n",
      "=============================================================================================\n",
      "\n",
      "Attempting to save best model weights...\n",
      "\tSave successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform training/validation\n",
    "distiller.run_main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CichlidDistillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
