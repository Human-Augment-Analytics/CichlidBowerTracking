{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_distillation.models.transformer.feature_extractors.triplet_cross_attention_vit import TripletCrossAttentionViT as TCAiT\n",
    "from data_distillation.models.transformer.feature_extractors.tcait_extractor import TCAiTExtractor\n",
    "from data_distillation.models.transformer.feature_extractors.pyramid.pyra_tcait import PyraTCAiT\n",
    "\n",
    "from data_distillation.losses.triplet_losses.triplet_classification_loss import TripletClassificationLoss as TCLoss\n",
    "from data_distillation.losses.triplet_losses.triplet_loss import TripletLoss\n",
    "\n",
    "from data_distillation.optimization.schedulers.warmup_cosine_scheduler import WarmupCosineScheduler\n",
    "\n",
    "from data_distillation.testing.data.test_triplets import TestTriplets\n",
    "from data_distillation.testing.data.triplets import Triplets\n",
    "from data_distillation.data_distiller import DataDistiller\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, RandomResizedCrop, CenterCrop, RandomHorizontalFlip, ColorJitter, Normalize, Lambda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug TCAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "NUM_CLASSES = 21841\n",
    "NUM_EXTRACTOR_HEADS = 8\n",
    "NUM_CLASSIFIER_HEADS = 8\n",
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "USE_MINIPATCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, extractor_use_minipatch=USE_MINIPATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    anchor = anchor.cuda()\n",
    "    positive = positive.cuda()\n",
    "    negative = negative.cuda()\n",
    "\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       393216 | (512, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                                                |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                                       |       100864 | (1, 197, 512)       \n",
      "positive_pos_encoder.pos_embedding                                     |       100864 | (1, 197, 512)       \n",
      "negative_pos_encoder.pos_embedding                                     |       100864 | (1, 197, 512)       \n",
      "transformer_blocks.0.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.0.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.0.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.0.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.0.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.0.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.0.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.0.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.0.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.0.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.0.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.1.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.1.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.1.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.1.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.1.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.1.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.1.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.1.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.1.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.2.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.2.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.2.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.2.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.2.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.2.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.2.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.2.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.2.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.3.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.3.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.3.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.3.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.3.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.3.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.3.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.3.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.3.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.4.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.4.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.4.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.4.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.4.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.4.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.4.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.4.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.4.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.5.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.5.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.5.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.5.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.5.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.5.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.5.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.5.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.5.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.6.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.6.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.6.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.6.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.6.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.6.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.6.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.6.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.6.mlp.2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.7.norm1.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.attention.in_proj_weight                          |       786432 | (1536, 512)         \n",
      "transformer_blocks.7.attention.in_proj_bias                            |         1536 | (1536,)             \n",
      "transformer_blocks.7.attention.out_proj.weight                         |       262144 | (512, 512)          \n",
      "transformer_blocks.7.attention.out_proj.bias                           |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.weight                                      |          512 | (512,)              \n",
      "transformer_blocks.7.norm2.bias                                        |          512 | (512,)              \n",
      "transformer_blocks.7.mlp.0.weight                                      |      1048576 | (2048, 512)         \n",
      "transformer_blocks.7.mlp.0.bias                                        |         2048 | (2048,)             \n",
      "transformer_blocks.7.mlp.2.weight                                      |      1048576 | (512, 2048)         \n",
      "transformer_blocks.7.mlp.2.bias                                        |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                                               |                            28020224\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | # Params     | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                                                  |          512 | (512,)              \n",
      "blocks.0.norm1.bias                                                    |          512 | (512,)              \n",
      "blocks.0.attention.in_proj_weight                                      |       786432 | (1536, 512)         \n",
      "blocks.0.attention.in_proj_bias                                        |         1536 | (1536,)             \n",
      "blocks.0.attention.out_proj.weight                                     |       262144 | (512, 512)          \n",
      "blocks.0.attention.out_proj.bias                                       |          512 | (512,)              \n",
      "blocks.0.norm2.weight                                                  |          512 | (512,)              \n",
      "blocks.0.norm2.bias                                                    |          512 | (512,)              \n",
      "blocks.0.mlp.0.weight                                                  |      1048576 | (2048, 512)         \n",
      "blocks.0.mlp.0.bias                                                    |         2048 | (2048,)             \n",
      "blocks.0.mlp.2.weight                                                  |      1048576 | (512, 2048)         \n",
      "blocks.0.mlp.2.bias                                                    |          512 | (512,)              \n",
      "blocks.1.norm1.weight                                                  |          512 | (512,)              \n",
      "blocks.1.norm1.bias                                                    |          512 | (512,)              \n",
      "blocks.1.attention.in_proj_weight                                      |       786432 | (1536, 512)         \n",
      "blocks.1.attention.in_proj_bias                                        |         1536 | (1536,)             \n",
      "blocks.1.attention.out_proj.weight                                     |       262144 | (512, 512)          \n",
      "blocks.1.attention.out_proj.bias                                       |          512 | (512,)              \n",
      "blocks.1.norm2.weight                                                  |          512 | (512,)              \n",
      "blocks.1.norm2.bias                                                    |          512 | (512,)              \n",
      "blocks.1.mlp.0.weight                                                  |      1048576 | (2048, 512)         \n",
      "blocks.1.mlp.0.bias                                                    |         2048 | (2048,)             \n",
      "blocks.1.mlp.2.weight                                                  |      1048576 | (512, 2048)         \n",
      "blocks.1.mlp.2.bias                                                    |          512 | (512,)              \n",
      "mlp.0.weight                                                           |      1048576 | (2048, 512)         \n",
      "mlp.0.bias                                                             |         2048 | (2048,)             \n",
      "mlp.1.weight                                                           |         2048 | (2048,)             \n",
      "mlp.1.bias                                                             |         2048 | (2048,)             \n",
      "mlp.3.weight                                                           |     16777216 | (8192, 2048)        \n",
      "mlp.3.bias                                                             |         8192 | (8192,)             \n",
      "mlp.4.weight                                                           |         8192 | (8192,)             \n",
      "mlp.4.bias                                                             |         8192 | (8192,)             \n",
      "mlp.6.weight                                                           |    178921472 | (21841, 8192)       \n",
      "mlp.6.bias                                                             |        21841 | (21841,)            \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                                              |                           203104593\n",
      "\n",
      "==============================================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS                                   |                           231124817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test string function\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and debug PyraTCAiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [64, 128, 320, 512]\n",
    "HEAD_COUNTS = [1, 2, 5, 8]\n",
    "MLP_RATIOS = [8, 8, 4, 4]\n",
    "SR_RATIOS = [8, 4, 2, 1]\n",
    "DEPTHS = [3, 3, 6, 3]\n",
    "ADD_CLASSIFIER = True\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialization\n",
    "\n",
    "model = PyraTCAiT(embed_dims=EMBED_DIMS, head_counts=HEAD_COUNTS, mlp_ratios=MLP_RATIOS, sr_ratios=SR_RATIOS, depths=DEPTHS,\n",
    "                  add_classifier=ADD_CLASSIFIER, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward function\n",
    "\n",
    "anchor = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "positive = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)\n",
    "negative = torch.rand(BATCH_SIZE, IMG_CHANNELS, IMG_DIM, IMG_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    anchor = anchor.cuda()\n",
    "    positive = positive.cuda()\n",
    "    negative = negative.cuda()\n",
    "\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_anchor, z_positive, z_negative, Y = model(anchor, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         3072 | (64, 3, 4, 4)       \n",
      "patcher.patch_conv.bias                                                |           64 | (64,)               \n",
      "patcher.norm.weight                                                    |           64 | (64,)               \n",
      "patcher.norm.bias                                                      |           64 | (64,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |       200704 | (1, 3136, 64)       \n",
      "positive_pos_encoder.pos_embedding                                     |       200704 | (1, 3136, 64)       \n",
      "negative_pos_encoder.pos_embedding                                     |       200704 | (1, 3136, 64)       \n",
      "transformer_stack.0.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.0.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.0.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.0.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.0.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.0.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.0.mlp.2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.1.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.1.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.1.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.1.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.1.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.1.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.1.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.1.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.1.mlp.2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.norm1.weight                                       |           64 | (64,)               \n",
      "transformer_stack.2.norm1.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |        12288 | (192, 64)           \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          192 | (192,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |         4096 | (64, 64)            \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |           64 | (64,)               \n",
      "transformer_stack.2.attention.sr.weight                                |       262144 | (64, 64, 8, 8)      \n",
      "transformer_stack.2.attention.sr.bias                                  |           64 | (64,)               \n",
      "transformer_stack.2.attention.norm.weight                              |           64 | (64,)               \n",
      "transformer_stack.2.attention.norm.bias                                |           64 | (64,)               \n",
      "transformer_stack.2.norm2.weight                                       |           64 | (64,)               \n",
      "transformer_stack.2.norm2.bias                                         |           64 | (64,)               \n",
      "transformer_stack.2.mlp.0.weight                                       |        32768 | (512, 64)           \n",
      "transformer_stack.2.mlp.0.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.mlp.2.weight                                       |        32768 | (64, 512)           \n",
      "transformer_stack.2.mlp.2.bias                                         |           64 | (64,)               \n",
      "positive_cross_attn.attention.in_proj_weight                           |        12288 | (192, 64)           \n",
      "positive_cross_attn.attention.in_proj_bias                             |          192 | (192,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |         4096 | (64, 64)            \n",
      "positive_cross_attn.attention.out_proj.bias                            |           64 | (64,)               \n",
      "positive_cross_attn.norm.weight                                        |           64 | (64,)               \n",
      "positive_cross_attn.norm.bias                                          |           64 | (64,)               \n",
      "negative_cross_attn.attention.in_proj_weight                           |        12288 | (192, 64)           \n",
      "negative_cross_attn.attention.in_proj_bias                             |          192 | (192,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |         4096 | (64, 64)            \n",
      "negative_cross_attn.attention.out_proj.bias                            |           64 | (64,)               \n",
      "negative_cross_attn.norm.weight                                        |           64 | (64,)               \n",
      "negative_cross_attn.norm.bias                                          |           64 | (64,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 0 TOTAL # PARAMS                                                 |                             1674944\n",
      "\n",
      "Stage 1\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        32768 | (128, 64, 2, 2)     \n",
      "patcher.patch_conv.bias                                                |          128 | (128,)              \n",
      "patcher.norm.weight                                                    |          128 | (128,)              \n",
      "patcher.norm.bias                                                      |          128 | (128,)              \n",
      "anchor_pos_encoder.pos_embedding                                       |       100352 | (1, 784, 128)       \n",
      "positive_pos_encoder.pos_embedding                                     |       100352 | (1, 784, 128)       \n",
      "negative_pos_encoder.pos_embedding                                     |       100352 | (1, 784, 128)       \n",
      "transformer_stack.0.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.0.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.0.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.0.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.0.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.0.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.0.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.0.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.1.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.1.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.1.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.1.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.1.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.1.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.1.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.norm1.weight                                       |          128 | (128,)              \n",
      "transformer_stack.2.norm1.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |        49152 | (384, 128)          \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          384 | (384,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |        16384 | (128, 128)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          128 | (128,)              \n",
      "transformer_stack.2.attention.sr.weight                                |       262144 | (128, 128, 4, 4)    \n",
      "transformer_stack.2.attention.sr.bias                                  |          128 | (128,)              \n",
      "transformer_stack.2.attention.norm.weight                              |          128 | (128,)              \n",
      "transformer_stack.2.attention.norm.bias                                |          128 | (128,)              \n",
      "transformer_stack.2.norm2.weight                                       |          128 | (128,)              \n",
      "transformer_stack.2.norm2.bias                                         |          128 | (128,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |       131072 | (1024, 128)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         1024 | (1024,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |       131072 | (128, 1024)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          128 | (128,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "positive_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "positive_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "negative_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "negative_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 1 TOTAL # PARAMS                                                 |                             2243968\n",
      "\n",
      "Stage 2\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       163840 | (320, 128, 2, 2)    \n",
      "patcher.patch_conv.bias                                                |          320 | (320,)              \n",
      "patcher.norm.weight                                                    |          320 | (320,)              \n",
      "patcher.norm.bias                                                      |          320 | (320,)              \n",
      "anchor_pos_encoder.pos_embedding                                       |        62720 | (1, 196, 320)       \n",
      "positive_pos_encoder.pos_embedding                                     |        62720 | (1, 196, 320)       \n",
      "negative_pos_encoder.pos_embedding                                     |        62720 | (1, 196, 320)       \n",
      "transformer_stack.0.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.0.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.0.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.0.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.0.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.0.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.0.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.0.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.1.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.1.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.1.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.1.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.1.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.1.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.1.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.2.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.2.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.2.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.2.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.2.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.2.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.2.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.3.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.3.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.3.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.3.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.3.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.3.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.3.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.3.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.3.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.3.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.3.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.3.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.3.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.3.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.4.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.4.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.4.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.4.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.4.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.4.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.4.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.4.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.4.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.4.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.4.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.4.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.4.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.4.mlp.2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.norm1.weight                                       |          320 | (320,)              \n",
      "transformer_stack.5.norm1.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.attention.attention.in_proj_weight                 |       307200 | (960, 320)          \n",
      "transformer_stack.5.attention.attention.in_proj_bias                   |          960 | (960,)              \n",
      "transformer_stack.5.attention.attention.out_proj.weight                |       102400 | (320, 320)          \n",
      "transformer_stack.5.attention.attention.out_proj.bias                  |          320 | (320,)              \n",
      "transformer_stack.5.attention.sr.weight                                |       409600 | (320, 320, 2, 2)    \n",
      "transformer_stack.5.attention.sr.bias                                  |          320 | (320,)              \n",
      "transformer_stack.5.attention.norm.weight                              |          320 | (320,)              \n",
      "transformer_stack.5.attention.norm.bias                                |          320 | (320,)              \n",
      "transformer_stack.5.norm2.weight                                       |          320 | (320,)              \n",
      "transformer_stack.5.norm2.bias                                         |          320 | (320,)              \n",
      "transformer_stack.5.mlp.0.weight                                       |       409600 | (1280, 320)         \n",
      "transformer_stack.5.mlp.0.bias                                         |         1280 | (1280,)             \n",
      "transformer_stack.5.mlp.2.weight                                       |       409600 | (320, 1280)         \n",
      "transformer_stack.5.mlp.2.bias                                         |          320 | (320,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       307200 | (960, 320)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          960 | (960,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |       102400 | (320, 320)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          320 | (320,)              \n",
      "positive_cross_attn.norm.weight                                        |          320 | (320,)              \n",
      "positive_cross_attn.norm.bias                                          |          320 | (320,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       307200 | (960, 320)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          960 | (960,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |       102400 | (320, 320)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          320 | (320,)              \n",
      "negative_cross_attn.norm.weight                                        |          320 | (320,)              \n",
      "negative_cross_attn.norm.bias                                          |          320 | (320,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 2 TOTAL # PARAMS                                                 |                            11037120\n",
      "\n",
      "Stage 3\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |       655360 | (512, 320, 2, 2)    \n",
      "patcher.patch_conv.bias                                                |          512 | (512,)              \n",
      "patcher.norm.weight                                                    |          512 | (512,)              \n",
      "patcher.norm.bias                                                      |          512 | (512,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          512 | (1, 1, 512)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          512 | (1, 1, 512)         \n",
      "anchor_pos_encoder.pos_embedding                                       |        25600 | (1, 50, 512)        \n",
      "positive_pos_encoder.pos_embedding                                     |        25600 | (1, 50, 512)        \n",
      "negative_pos_encoder.pos_embedding                                     |        25600 | (1, 50, 512)        \n",
      "transformer_stack.0.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.0.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.0.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.0.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.0.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.0.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.0.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.0.mlp.2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.1.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.1.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.1.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.1.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.1.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.1.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.1.mlp.2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.norm1.weight                                       |          512 | (512,)              \n",
      "transformer_stack.2.norm1.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |       786432 | (1536, 512)         \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |         1536 | (1536,)             \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |       262144 | (512, 512)          \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |          512 | (512,)              \n",
      "transformer_stack.2.norm2.weight                                       |          512 | (512,)              \n",
      "transformer_stack.2.norm2.bias                                         |          512 | (512,)              \n",
      "transformer_stack.2.mlp.0.weight                                       |      1048576 | (2048, 512)         \n",
      "transformer_stack.2.mlp.0.bias                                         |         2048 | (2048,)             \n",
      "transformer_stack.2.mlp.2.weight                                       |      1048576 | (512, 2048)         \n",
      "transformer_stack.2.mlp.2.bias                                         |          512 | (512,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "positive_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "positive_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "positive_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "positive_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |       786432 | (1536, 512)         \n",
      "negative_cross_attn.attention.in_proj_bias                             |         1536 | (1536,)             \n",
      "negative_cross_attn.attention.out_proj.weight                          |       262144 | (512, 512)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          512 | (512,)              \n",
      "negative_cross_attn.norm.weight                                        |          512 | (512,)              \n",
      "negative_cross_attn.norm.bias                                          |          512 | (512,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 3 TOTAL # PARAMS                                                 |                            12295680\n",
      "\n",
      "==============================================================================================================\n",
      "Pre-classifier PyraT-CAiT # PARAMS                                     |                            27251712\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "weight                                                                 |         1024 | (2, 512)            \n",
      "bias                                                                   |            2 | (2,)                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CLASSIFIER # PARAMS                                                    |                                1026\n",
      "\n",
      "==============================================================================================================\n",
      "FULL PyraT-CAiT # PARAMS                                               |                            27252738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test string function\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataDistiller object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "NUM_CLASSES = 2\n",
    "NUM_EXTRACTOR_HEADS = 2\n",
    "NUM_CLASSIFIER_HEADS = 2\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_BATCHES = 4\n",
    "NUM_VALID_BATCHES = 1\n",
    "IMG_CHANNELS = 3\n",
    "IMG_DIM = 224\n",
    "EXTRACTOR_DEPTH = 4\n",
    "EXTRACTOR_MLP_RATIO = 2.0\n",
    "CLASSIFIER_DEPTH = 1\n",
    "CLASSIFIER_MLP_RATIO = 2.0\n",
    "USE_MINIPATCH = False\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "CHECKPOINTS_DIR = '/Users/charlieclark/Documents/GATech/OMSCS/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/testing_weights'\n",
    "DEVICE = 'cpu'\n",
    "GPU_ID = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        98304 | (128, 3, 16, 16)    \n",
      "patcher.patch_conv.bias                                                |          128 | (128,)              \n",
      "anchor_cls_tokenizer.cls_tokens                                        |          128 | (1, 1, 128)         \n",
      "positive_cls_tokenizer.cls_tokens                                      |          128 | (1, 1, 128)         \n",
      "negative_cls_tokenizer.cls_tokens                                      |          128 | (1, 1, 128)         \n",
      "anchor_pos_encoder.pos_embedding                                       |        25216 | (1, 197, 128)       \n",
      "positive_pos_encoder.pos_embedding                                     |        25216 | (1, 197, 128)       \n",
      "negative_pos_encoder.pos_embedding                                     |        25216 | (1, 197, 128)       \n",
      "transformer_blocks.0.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.0.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.0.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.0.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.0.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.0.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.0.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.0.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.0.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.0.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.0.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.1.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.1.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.1.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.1.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.1.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.1.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.1.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.1.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.1.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.2.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.2.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.2.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.2.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.2.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.2.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.2.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.2.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.2.mlp.2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.3.norm1.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.attention.in_proj_weight                          |        49152 | (384, 128)          \n",
      "transformer_blocks.3.attention.in_proj_bias                            |          384 | (384,)              \n",
      "transformer_blocks.3.attention.out_proj.weight                         |        16384 | (128, 128)          \n",
      "transformer_blocks.3.attention.out_proj.bias                           |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.weight                                      |          128 | (128,)              \n",
      "transformer_blocks.3.norm2.bias                                        |          128 | (128,)              \n",
      "transformer_blocks.3.mlp.0.weight                                      |        32768 | (256, 128)          \n",
      "transformer_blocks.3.mlp.0.bias                                        |          256 | (256,)              \n",
      "transformer_blocks.3.mlp.2.weight                                      |        32768 | (128, 256)          \n",
      "transformer_blocks.3.mlp.2.bias                                        |          128 | (128,)              \n",
      "positive_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "positive_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "positive_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "positive_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "positive_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "positive_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "negative_cross_attn.attention.in_proj_weight                           |        49152 | (384, 128)          \n",
      "negative_cross_attn.attention.in_proj_bias                             |          384 | (384,)              \n",
      "negative_cross_attn.attention.out_proj.weight                          |        16384 | (128, 128)          \n",
      "negative_cross_attn.attention.out_proj.bias                            |          128 | (128,)              \n",
      "negative_cross_attn.norm.weight                                        |          128 | (128,)              \n",
      "negative_cross_attn.norm.bias                                          |          128 | (128,)              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL EXTRACTOR # PARAMS                                               |                              836992\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | # Params     | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "blocks.0.norm1.weight                                                  |          128 | (128,)              \n",
      "blocks.0.norm1.bias                                                    |          128 | (128,)              \n",
      "blocks.0.attention.in_proj_weight                                      |        49152 | (384, 128)          \n",
      "blocks.0.attention.in_proj_bias                                        |          384 | (384,)              \n",
      "blocks.0.attention.out_proj.weight                                     |        16384 | (128, 128)          \n",
      "blocks.0.attention.out_proj.bias                                       |          128 | (128,)              \n",
      "blocks.0.norm2.weight                                                  |          128 | (128,)              \n",
      "blocks.0.norm2.bias                                                    |          128 | (128,)              \n",
      "blocks.0.mlp.0.weight                                                  |        32768 | (256, 128)          \n",
      "blocks.0.mlp.0.bias                                                    |          256 | (256,)              \n",
      "blocks.0.mlp.2.weight                                                  |        32768 | (128, 256)          \n",
      "blocks.0.mlp.2.bias                                                    |          128 | (128,)              \n",
      "mlp.0.weight                                                           |        32768 | (256, 128)          \n",
      "mlp.0.bias                                                             |          256 | (256,)              \n",
      "mlp.1.weight                                                           |          256 | (256,)              \n",
      "mlp.1.bias                                                             |          256 | (256,)              \n",
      "mlp.3.weight                                                           |       131072 | (512, 256)          \n",
      "mlp.3.bias                                                             |          512 | (512,)              \n",
      "mlp.4.weight                                                           |          512 | (512,)              \n",
      "mlp.4.bias                                                             |          512 | (512,)              \n",
      "mlp.6.weight                                                           |         1024 | (2, 512)            \n",
      "mlp.6.bias                                                             |            2 | (2,)                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "TOTAL CLASSIFIER # PARAMS                                              |                              299650\n",
      "\n",
      "==============================================================================================================\n",
      "TRIPLET CROSS ATTENTION ViT # PARAMS                                   |                             1136642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup simple T-CAiT model\n",
    "model = TCAiT(embed_dim=EMBED_DIM, num_classes=NUM_CLASSES, num_extractor_heads=NUM_EXTRACTOR_HEADS, num_classifier_heads=NUM_CLASSIFIER_HEADS, in_channels=IMG_CHANNELS, in_dim=IMG_DIM, \\\n",
    "              extractor_depth=EXTRACTOR_DEPTH, extractor_mlp_ratio=EXTRACTOR_MLP_RATIO, classifier_depth=CLASSIFIER_DEPTH, classifier_mlp_ratio=CLASSIFIER_MLP_RATIO, extractor_use_minipatch=USE_MINIPATCH)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets and dataloaders\n",
    "train_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_TRAIN_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_VALID_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function\n",
    "loss_fn = TCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datadistiller\n",
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, nepochs=NUM_EPOCHS, nclasses=NUM_CLASSES, checkpoints_dir=CHECKPOINTS_DIR,device=DEVICE, gpu_id=GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:12<00:00,  1.50it/s, accuracy=0.544, loss=12.2]\n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:05<00:00,  3.52it/s, accuracy=0.439, loss=5.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Checkpoint Saved!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:08<00:00,  2.15it/s, accuracy=0.508, loss=4.1] \n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:05<00:00,  3.51it/s, accuracy=0.479, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Checkpoint Saved!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [2/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:09<00:00,  2.03it/s, accuracy=0.448, loss=1.06]\n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:05<00:00,  3.47it/s, accuracy=0.491, loss=0.946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Checkpoint Saved!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [3/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:09<00:00,  1.92it/s, accuracy=0.52, loss=0.948] \n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:05<00:00,  3.20it/s, accuracy=0.464, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Checkpoint Saved!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [4/5]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:09<00:00,  1.90it/s, accuracy=0.456, loss=1.08]\n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:05<00:00,  3.36it/s, accuracy=0.462, loss=0.939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Checkpoint Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform training/validation\n",
    "distiller.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [12, 24, 48, 96]\n",
    "HEAD_COUNTS = [1, 2, 4, 6]\n",
    "MLP_RATIOS = [4, 4, 2, 2]\n",
    "SR_RATIOS = [8, 4, 2, 1]\n",
    "DEPTHS = [1, 2, 4, 2]\n",
    "ADD_CLASSIFIER = True\n",
    "NUM_CLASSES = 2\n",
    "INIT_ALPHA = 0.1\n",
    "INIT_BETA = 0.1\n",
    "USE_IMPROVED = True\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "CHECKPOINTS_DIR = '/Users/charlieclark/Documents/GATech/OMSCS/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/testing_weights'\n",
    "DEVICE = 'cpu'\n",
    "GPU_ID = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |          576 | (12, 3, 4, 4)       \n",
      "patcher.patch_conv.bias                                                |           12 | (12,)               \n",
      "patcher.norm.weight                                                    |           12 | (12,)               \n",
      "patcher.norm.bias                                                      |           12 | (12,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |        37632 | (1, 3136, 12)       \n",
      "positive_pos_encoder.pos_embedding                                     |        37632 | (1, 3136, 12)       \n",
      "negative_pos_encoder.pos_embedding                                     |        37632 | (1, 3136, 12)       \n",
      "tca_block.alpha                                                        |        37632 | (1, 3136, 12)       \n",
      "tca_block.beta                                                         |        37632 | (1, 3136, 12)       \n",
      "tca_block.norm1.weight                                                 |           12 | (12,)               \n",
      "tca_block.norm1.bias                                                   |           12 | (12,)               \n",
      "tca_block.positive_cross_attn.attention.in_proj_weight                 |          432 | (36, 12)            \n",
      "tca_block.positive_cross_attn.attention.in_proj_bias                   |           36 | (36,)               \n",
      "tca_block.positive_cross_attn.attention.out_proj.weight                |          144 | (12, 12)            \n",
      "tca_block.positive_cross_attn.attention.out_proj.bias                  |           12 | (12,)               \n",
      "tca_block.positive_cross_attn.norm.weight                              |           12 | (12,)               \n",
      "tca_block.positive_cross_attn.norm.bias                                |           12 | (12,)               \n",
      "tca_block.negative_cross_attn.attention.in_proj_weight                 |          432 | (36, 12)            \n",
      "tca_block.negative_cross_attn.attention.in_proj_bias                   |           36 | (36,)               \n",
      "tca_block.negative_cross_attn.attention.out_proj.weight                |          144 | (12, 12)            \n",
      "tca_block.negative_cross_attn.attention.out_proj.bias                  |           12 | (12,)               \n",
      "tca_block.negative_cross_attn.norm.weight                              |           12 | (12,)               \n",
      "tca_block.negative_cross_attn.norm.bias                                |           12 | (12,)               \n",
      "tca_block.norm2.weight                                                 |           12 | (12,)               \n",
      "tca_block.norm2.bias                                                   |           12 | (12,)               \n",
      "tca_block.mlp.0.weight                                                 |          576 | (48, 12)            \n",
      "tca_block.mlp.0.bias                                                   |           48 | (48,)               \n",
      "tca_block.mlp.2.weight                                                 |          576 | (12, 48)            \n",
      "tca_block.mlp.2.bias                                                   |           12 | (12,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 0 TOTAL # PARAMS                                                 |                              191328\n",
      "\n",
      "Stage 1\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         1152 | (24, 12, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           24 | (24,)               \n",
      "patcher.norm.weight                                                    |           24 | (24,)               \n",
      "patcher.norm.bias                                                      |           24 | (24,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |        18816 | (1, 784, 24)        \n",
      "positive_pos_encoder.pos_embedding                                     |        18816 | (1, 784, 24)        \n",
      "negative_pos_encoder.pos_embedding                                     |        18816 | (1, 784, 24)        \n",
      "transformer_stack.0.norm1.weight                                       |           24 | (24,)               \n",
      "transformer_stack.0.norm1.bias                                         |           24 | (24,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |         1728 | (72, 24)            \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |           72 | (72,)               \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |          576 | (24, 24)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           24 | (24,)               \n",
      "transformer_stack.0.attention.sr.weight                                |         9216 | (24, 24, 4, 4)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           24 | (24,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           24 | (24,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           24 | (24,)               \n",
      "transformer_stack.0.norm2.weight                                       |           24 | (24,)               \n",
      "transformer_stack.0.norm2.bias                                         |           24 | (24,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |         2304 | (96, 24)            \n",
      "transformer_stack.0.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.2.weight                                       |         2304 | (24, 96)            \n",
      "transformer_stack.0.mlp.2.bias                                         |           24 | (24,)               \n",
      "tca_block.alpha                                                        |        18816 | (1, 784, 24)        \n",
      "tca_block.beta                                                         |        18816 | (1, 784, 24)        \n",
      "tca_block.norm1.weight                                                 |           24 | (24,)               \n",
      "tca_block.norm1.bias                                                   |           24 | (24,)               \n",
      "tca_block.positive_cross_attn.attention.in_proj_weight                 |         1728 | (72, 24)            \n",
      "tca_block.positive_cross_attn.attention.in_proj_bias                   |           72 | (72,)               \n",
      "tca_block.positive_cross_attn.attention.out_proj.weight                |          576 | (24, 24)            \n",
      "tca_block.positive_cross_attn.attention.out_proj.bias                  |           24 | (24,)               \n",
      "tca_block.positive_cross_attn.norm.weight                              |           24 | (24,)               \n",
      "tca_block.positive_cross_attn.norm.bias                                |           24 | (24,)               \n",
      "tca_block.negative_cross_attn.attention.in_proj_weight                 |         1728 | (72, 24)            \n",
      "tca_block.negative_cross_attn.attention.in_proj_bias                   |           72 | (72,)               \n",
      "tca_block.negative_cross_attn.attention.out_proj.weight                |          576 | (24, 24)            \n",
      "tca_block.negative_cross_attn.attention.out_proj.bias                  |           24 | (24,)               \n",
      "tca_block.negative_cross_attn.norm.weight                              |           24 | (24,)               \n",
      "tca_block.negative_cross_attn.norm.bias                                |           24 | (24,)               \n",
      "tca_block.norm2.weight                                                 |           24 | (24,)               \n",
      "tca_block.norm2.bias                                                   |           24 | (24,)               \n",
      "tca_block.mlp.0.weight                                                 |         2304 | (96, 24)            \n",
      "tca_block.mlp.0.bias                                                   |           96 | (96,)               \n",
      "tca_block.mlp.2.weight                                                 |         2304 | (24, 96)            \n",
      "tca_block.mlp.2.bias                                                   |           24 | (24,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 1 TOTAL # PARAMS                                                 |                              121536\n",
      "\n",
      "Stage 2\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |         4608 | (48, 24, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           48 | (48,)               \n",
      "patcher.norm.weight                                                    |           48 | (48,)               \n",
      "patcher.norm.bias                                                      |           48 | (48,)               \n",
      "anchor_pos_encoder.pos_embedding                                       |         9408 | (1, 196, 48)        \n",
      "positive_pos_encoder.pos_embedding                                     |         9408 | (1, 196, 48)        \n",
      "negative_pos_encoder.pos_embedding                                     |         9408 | (1, 196, 48)        \n",
      "transformer_stack.0.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.0.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.0.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.0.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.0.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.0.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.0.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.0.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.0.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.0.mlp.2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.1.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.1.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.1.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.1.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.1.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.1.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.1.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.1.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.1.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.1.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.1.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.1.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.1.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.1.mlp.2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.norm1.weight                                       |           48 | (48,)               \n",
      "transformer_stack.2.norm1.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.attention.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "transformer_stack.2.attention.attention.in_proj_bias                   |          144 | (144,)              \n",
      "transformer_stack.2.attention.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "transformer_stack.2.attention.attention.out_proj.bias                  |           48 | (48,)               \n",
      "transformer_stack.2.attention.sr.weight                                |         9216 | (48, 48, 2, 2)      \n",
      "transformer_stack.2.attention.sr.bias                                  |           48 | (48,)               \n",
      "transformer_stack.2.attention.norm.weight                              |           48 | (48,)               \n",
      "transformer_stack.2.attention.norm.bias                                |           48 | (48,)               \n",
      "transformer_stack.2.norm2.weight                                       |           48 | (48,)               \n",
      "transformer_stack.2.norm2.bias                                         |           48 | (48,)               \n",
      "transformer_stack.2.mlp.0.weight                                       |         4608 | (96, 48)            \n",
      "transformer_stack.2.mlp.0.bias                                         |           96 | (96,)               \n",
      "transformer_stack.2.mlp.2.weight                                       |         4608 | (48, 96)            \n",
      "transformer_stack.2.mlp.2.bias                                         |           48 | (48,)               \n",
      "tca_block.alpha                                                        |         9408 | (1, 196, 48)        \n",
      "tca_block.beta                                                         |         9408 | (1, 196, 48)        \n",
      "tca_block.norm1.weight                                                 |           48 | (48,)               \n",
      "tca_block.norm1.bias                                                   |           48 | (48,)               \n",
      "tca_block.positive_cross_attn.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "tca_block.positive_cross_attn.attention.in_proj_bias                   |          144 | (144,)              \n",
      "tca_block.positive_cross_attn.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "tca_block.positive_cross_attn.attention.out_proj.bias                  |           48 | (48,)               \n",
      "tca_block.positive_cross_attn.norm.weight                              |           48 | (48,)               \n",
      "tca_block.positive_cross_attn.norm.bias                                |           48 | (48,)               \n",
      "tca_block.negative_cross_attn.attention.in_proj_weight                 |         6912 | (144, 48)           \n",
      "tca_block.negative_cross_attn.attention.in_proj_bias                   |          144 | (144,)              \n",
      "tca_block.negative_cross_attn.attention.out_proj.weight                |         2304 | (48, 48)            \n",
      "tca_block.negative_cross_attn.attention.out_proj.bias                  |           48 | (48,)               \n",
      "tca_block.negative_cross_attn.norm.weight                              |           48 | (48,)               \n",
      "tca_block.negative_cross_attn.norm.bias                                |           48 | (48,)               \n",
      "tca_block.norm2.weight                                                 |           48 | (48,)               \n",
      "tca_block.norm2.bias                                                   |           48 | (48,)               \n",
      "tca_block.mlp.0.weight                                                 |         4608 | (96, 48)            \n",
      "tca_block.mlp.0.bias                                                   |           96 | (96,)               \n",
      "tca_block.mlp.2.weight                                                 |         4608 | (48, 96)            \n",
      "tca_block.mlp.2.bias                                                   |           48 | (48,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 2 TOTAL # PARAMS                                                 |                              165312\n",
      "\n",
      "Stage 3\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "patcher.patch_conv.weight                                              |        18432 | (96, 48, 2, 2)      \n",
      "patcher.patch_conv.bias                                                |           96 | (96,)               \n",
      "patcher.norm.weight                                                    |           96 | (96,)               \n",
      "patcher.norm.bias                                                      |           96 | (96,)               \n",
      "anchor_cls_tokenizer.cls_tokens                                        |           96 | (1, 1, 96)          \n",
      "positive_cls_tokenizer.cls_tokens                                      |           96 | (1, 1, 96)          \n",
      "negative_cls_tokenizer.cls_tokens                                      |           96 | (1, 1, 96)          \n",
      "anchor_pos_encoder.pos_embedding                                       |         4800 | (1, 50, 96)         \n",
      "positive_pos_encoder.pos_embedding                                     |         4800 | (1, 50, 96)         \n",
      "negative_pos_encoder.pos_embedding                                     |         4800 | (1, 50, 96)         \n",
      "transformer_stack.0.norm1.weight                                       |           96 | (96,)               \n",
      "transformer_stack.0.norm1.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.attention.attention.in_proj_weight                 |        27648 | (288, 96)           \n",
      "transformer_stack.0.attention.attention.in_proj_bias                   |          288 | (288,)              \n",
      "transformer_stack.0.attention.attention.out_proj.weight                |         9216 | (96, 96)            \n",
      "transformer_stack.0.attention.attention.out_proj.bias                  |           96 | (96,)               \n",
      "transformer_stack.0.norm2.weight                                       |           96 | (96,)               \n",
      "transformer_stack.0.norm2.bias                                         |           96 | (96,)               \n",
      "transformer_stack.0.mlp.0.weight                                       |        18432 | (192, 96)           \n",
      "transformer_stack.0.mlp.0.bias                                         |          192 | (192,)              \n",
      "transformer_stack.0.mlp.2.weight                                       |        18432 | (96, 192)           \n",
      "transformer_stack.0.mlp.2.bias                                         |           96 | (96,)               \n",
      "tca_block.alpha                                                        |         4800 | (1, 50, 96)         \n",
      "tca_block.beta                                                         |         4800 | (1, 50, 96)         \n",
      "tca_block.norm1.weight                                                 |           96 | (96,)               \n",
      "tca_block.norm1.bias                                                   |           96 | (96,)               \n",
      "tca_block.positive_cross_attn.attention.in_proj_weight                 |        27648 | (288, 96)           \n",
      "tca_block.positive_cross_attn.attention.in_proj_bias                   |          288 | (288,)              \n",
      "tca_block.positive_cross_attn.attention.out_proj.weight                |         9216 | (96, 96)            \n",
      "tca_block.positive_cross_attn.attention.out_proj.bias                  |           96 | (96,)               \n",
      "tca_block.positive_cross_attn.norm.weight                              |           96 | (96,)               \n",
      "tca_block.positive_cross_attn.norm.bias                                |           96 | (96,)               \n",
      "tca_block.negative_cross_attn.attention.in_proj_weight                 |        27648 | (288, 96)           \n",
      "tca_block.negative_cross_attn.attention.in_proj_bias                   |          288 | (288,)              \n",
      "tca_block.negative_cross_attn.attention.out_proj.weight                |         9216 | (96, 96)            \n",
      "tca_block.negative_cross_attn.attention.out_proj.bias                  |           96 | (96,)               \n",
      "tca_block.negative_cross_attn.norm.weight                              |           96 | (96,)               \n",
      "tca_block.negative_cross_attn.norm.bias                                |           96 | (96,)               \n",
      "tca_block.norm2.weight                                                 |           96 | (96,)               \n",
      "tca_block.norm2.bias                                                   |           96 | (96,)               \n",
      "tca_block.mlp.0.weight                                                 |        18432 | (192, 96)           \n",
      "tca_block.mlp.0.bias                                                   |          192 | (192,)              \n",
      "tca_block.mlp.2.weight                                                 |        18432 | (96, 192)           \n",
      "tca_block.mlp.2.bias                                                   |           96 | (96,)               \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "STAGE 3 TOTAL # PARAMS                                                 |                              230208\n",
      "\n",
      "==============================================================================================================\n",
      "Pre-classifier PyraT-CAiT # PARAMS                                     |                              708384\n",
      "\n",
      "Classifier\n",
      "==============================================================================================================\n",
      "Name                                                                   | Params       | Size                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "weight                                                                 |          192 | (2, 96)             \n",
      "bias                                                                   |            2 | (2,)                \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CLASSIFIER # PARAMS                                                    |                                 194\n",
      "\n",
      "==============================================================================================================\n",
      "FULL PyraT-CAiT # PARAMS                                               |                              708578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setup simple PyraT-CAiT model\n",
    "model = PyraTCAiT(embed_dims=EMBED_DIMS, head_counts=HEAD_COUNTS, mlp_ratios=MLP_RATIOS, sr_ratios=SR_RATIOS, depths=DEPTHS,\n",
    "                  add_classifier=ADD_CLASSIFIER, num_classes=NUM_CLASSES, init_alpha=INIT_ALPHA, init_beta=INIT_BETA, use_improved=USE_IMPROVED)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets and dataloaders\n",
    "train_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_TRAIN_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TestTriplets(batch_size=BATCH_SIZE, num_batches=NUM_VALID_BATCHES, num_channels=IMG_CHANNELS, dim=IMG_DIM)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup loss function\n",
    "loss_fn = TCLoss() if ADD_CLASSIFIER else TripletLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datadistiller\n",
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, nepochs=NUM_EPOCHS, nclasses=NUM_CLASSES, checkpoints_dir=CHECKPOINTS_DIR, device=DEVICE, gpu_id=GPU_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/2]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [02:08<00:00,  6.74s/it, accuracy=0.488, loss=0.73] \n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:28<00:00,  1.49s/it, accuracy=0.465, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Checkpoint Saved!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/2]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [18/19]: 100%|██████████| 19/19 [00:58<00:00,  3.09s/it, accuracy=0.491, loss=0.697]\n",
      "Validation, Batch [18/19]: 100%|██████████| 19/19 [00:27<00:00,  1.47s/it, accuracy=0.492, loss=0.696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Checkpoint Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform training/validation\n",
    "distiller.main_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset paths\n",
    "DATASET_PATH = '/home/hice1/cclark339/scratch/Data/ImageNet-1K/imagenet1k-triplets.csv'\n",
    "\n",
    "# data splitting args\n",
    "NUM_CLASSES = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.1\n",
    "SHUFFLE = True\n",
    "\n",
    "# data augmentation args\n",
    "TRAIN_DIM = 224\n",
    "VALID_DIM = 256\n",
    "\n",
    "BRIGHTNESS = 0.4\n",
    "CONTRAST = 0.4\n",
    "SATURATION = 0.4\n",
    "HUE = 0.1\n",
    "\n",
    "NORM_MEANS = [0.485, 0.456, 0.406]\n",
    "NORM_STDS = [0.229, 0.224, 0.225]\n",
    "\n",
    "# setup args\n",
    "BATCH_SIZE = 16\n",
    "CHANNELS = 3\n",
    "NUM_EPOCHS = 10\n",
    "CHECKPOINTS_DIR = '/home/hice1/cclark339/ondemand/CichlidBowerTracking/cichlid_bower_tracking/data_distillation/models/testing_weights'\n",
    "DEVICE = 'gpu'\n",
    "GPU_ID = 0\n",
    "START_EPOCH = 0\n",
    "USE_DDP = False\n",
    "DISABLE_PROGRESS_BAR = False\n",
    "\n",
    "# model args\n",
    "EMBED_DIMS = [64, 128, 320, 512]\n",
    "HEAD_COUNTS = [1, 2, 5, 8]\n",
    "MLP_RATIOS = [8, 8, 4, 4]\n",
    "SR_RATIOS = [8, 4, 2, 1]\n",
    "DEPTHS = [3, 3, 6, 3]\n",
    "PATCH_SIZE = 4\n",
    "NUM_STAGES = 4\n",
    "DROPOUT = 0.1\n",
    "USE_IMPROVED = True\n",
    "CLASSIFICATION_INTENT = True\n",
    "\n",
    "# optimizer and scheduler args\n",
    "USE_LABEL_SMOOTHING = False\n",
    "LEARNING_RATE = 1e-4\n",
    "BETAS = [0.9, 0.999]\n",
    "WEIGHT_DECAY = 2.5e-4\n",
    "PATIENCE = 10\n",
    "WARMUP_EPOCHS = 5\n",
    "ETA_MIN = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>/home/hice1/cclark339/scratch/Data/ImageNet-1K...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                anchor  \\\n",
       "18   /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "291  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "321  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "450  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "641  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "\n",
       "                                              positive  \\\n",
       "18   /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "291  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "321  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "450  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "641  /home/hice1/cclark339/scratch/Data/ImageNet-1K...   \n",
       "\n",
       "                                              negative  label  \n",
       "18   /home/hice1/cclark339/scratch/Data/ImageNet-1K...      3  \n",
       "291  /home/hice1/cclark339/scratch/Data/ImageNet-1K...      4  \n",
       "321  /home/hice1/cclark339/scratch/Data/ImageNet-1K...      2  \n",
       "450  /home/hice1/cclark339/scratch/Data/ImageNet-1K...      3  \n",
       "641  /home/hice1/cclark339/scratch/Data/ImageNet-1K...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df[df['label'] < NUM_CLASSES]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = [\n",
    "    Lambda(lambda img: img.repeat(3, 1, 1) if img.shape[0] == 1 else img),\n",
    "    RandomResizedCrop(TRAIN_DIM),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJitter(BRIGHTNESS, CONTRAST, SATURATION, HUE),\n",
    "    Normalize(NORM_MEANS, NORM_STDS)\n",
    "]\n",
    "\n",
    "valid_transform = [\n",
    "    Lambda(lambda img: img.repeat(3, 1, 1) if img.shape[0] == 1 else img),\n",
    "    Resize(VALID_DIM),\n",
    "    CenterCrop(TRAIN_DIM),\n",
    "    Normalize(NORM_MEANS, NORM_STDS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=SHUFFLE)\n",
    "del df\n",
    "\n",
    "train_dataset = Triplets(train_df, transform=Compose(train_transform))\n",
    "valid_dataset = Triplets(valid_df, transform=Compose(valid_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyraTCAiT(embed_dims=EMBED_DIMS, head_counts=HEAD_COUNTS, mlp_ratios=MLP_RATIOS, sr_ratios=SR_RATIOS, depths=DEPTHS,\n",
    "                  num_stages=NUM_STAGES, dropout=DROPOUT, first_patch_dim=PATCH_SIZE, in_channels=CHANNELS, in_dim=TRAIN_DIM, \n",
    "                  add_classifier=CLASSIFICATION_INTENT, use_improved=USE_IMPROVED, classification_intent=CLASSIFICATION_INTENT, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=tuple(BETAS), weight_decay=WEIGHT_DECAY)\n",
    "scheduler = WarmupCosineScheduler(optimizer=optimizer, warmup_epochs=WARMUP_EPOCHS, total_epochs=NUM_EPOCHS, eta_min=ETA_MIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = TCLoss(use_label_smoothing=USE_LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller = DataDistiller(train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler, nepochs=NUM_EPOCHS, \n",
    "                          nclasses=NUM_CLASSES, checkpoints_dir=CHECKPOINTS_DIR, device=DEVICE, gpu_id=GPU_ID, start_epoch=START_EPOCH, ddp=USE_DDP, disable_progress_bar=DISABLE_PROGRESS_BAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [0/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/366 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:18<00:00,  1.85it/s, accuracy=0.185, loss=2.36]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.29it/s, accuracy=0.199, loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [1/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:18<00:00,  1.85it/s, accuracy=0.201, loss=1.63]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.45it/s, accuracy=0.171, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [2/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:17<00:00,  1.85it/s, accuracy=0.212, loss=1.61]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.41it/s, accuracy=0.269, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [3/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:18<00:00,  1.84it/s, accuracy=0.239, loss=1.66]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.43it/s, accuracy=0.227, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [4/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:18<00:00,  1.85it/s, accuracy=0.201, loss=1.61]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.29it/s, accuracy=0.171, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [5/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:22<00:00,  1.80it/s, accuracy=0.2, loss=1.61]  \n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.26it/s, accuracy=0.171, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [6/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:23<00:00,  1.80it/s, accuracy=0.203, loss=1.61]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.35it/s, accuracy=0.171, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [7/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [365/366]: 100%|██████████| 366/366 [03:24<00:00,  1.79it/s, accuracy=0.207, loss=1.61]\n",
      "Validation, Batch [40/41]: 100%|██████████| 41/41 [00:09<00:00,  4.26it/s, accuracy=0.229, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Checkpoint Saved!\n",
      "Previous Epoch's Checkpoint Loaded!\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "EPOCH [8/10]\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, Batch [343/366]:  94%|█████████▍| 344/366 [03:09<00:11,  1.84it/s, accuracy=0.196, loss=1.61]"
     ]
    }
   ],
   "source": [
    "distiller.main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
